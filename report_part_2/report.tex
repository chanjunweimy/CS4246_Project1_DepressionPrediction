\documentclass{article}
\usepackage{aaai}
\usepackage{fixbib}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{url}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{FB01}{fi}
\graphicspath{ {images/} }
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}


\title{
	CS4246 AI Planning and Decision Making - Project 2 \\
	Planning and Decision Making Automation on Depression
}
\author{
	{\bf Team 01} \\
	Antoine Charles Vincent Garcia - A0159072A\\
	Chan Jun Wei - A0112084U\\
	Chen Tze Cheng - A0112092W\\
	Eric Ewe Yow Choong - A0112204E\\
	Han Liang Wee, Eric - A0065517A\\
	Ho Wei Li - A0094679H\\
}

\begin{document}
 	\maketitle

	\begin{abstract}
	\begin{quote}
	Depression is a debilitating mental illness that has seen the number of medically diagnosed cases increase at an alarming rate. Moreover, suicide rates, an indicator of major depression, is on the rise in the world. Medical schools are not producing enough of psychiatrists, leading to the potential problem that there are not enough psychiatrists (supply) in the world to meet demand, in particular depression cases. In our project, we propose a general framework to pre-screen patients before they make appointments to reduce workload on clinics. In particular, pre-screen patients who are coming in because of depression. The framework relies on Gaussian Process to decide if the patient require an appointment and urgency of the case. In addition, we produced a proof-of-concept prototype illustrating our framework. We also further note that the framework can be extended to other psychiatric disorders. 
\\
	\end{quote}
	\end{abstract}
	
	\section{Introduction}
	Depression has a severe, and at times long-term, negative impact on an individual's quality of life. 
	Major depression is 3rd leading cause of disability worldwide with 65 million life years spent living with the disability or lost due to early death \cite{who2004}.
	Depression's annual toll on U.S businesses amounts to about \$80 billion in medical expenditures, lost productivity and suicide. 
	Among the costs, close to \$10 billion accrued in lost workdays each year and more than \$33 billion in other costs accrue from decreased productivity due to symptoms that sap energy, affect work habits, cause problems with concentration, memory, and decision-making \cite{tjcp2015}. \\

	Left unchecked, depression increases risk for morbidity, suicide, decreased cognitive and social functioning, self-neglect, and early death \cite{arcp2009}. 
	Death from suicide is one of the top 10 causes of death, above the death rate for chronic liver disease, Alzheimer's, homicide, arteriosclerosis or hypertension \cite{nvsr2016}. \\

	General practioners (GPs) are oftern dubbed as the first line in the provision of healthcare as they form the interface between the healthcare system and patients. It is no exception for cases of depression where about half of those with 80\% of people diagnosed with depression are treated entirely by GPs \cite{goldbergd1994}. 

	However, during a visit to the GP, a patient's symptoms that are not physical in nature may not be inquired or brought up. Even if some GPs show interest in patients' psychosocial issues, they likely lack the requisite interviewing skills to properly elicit the relevant history while others lack the sensitivity to affective and nonverbal patient cues \cite{badger1994}. In addition, GPs have a plethora of tasks during each consultation such as assessment and management of ongoing general medical issues, prevention and health maintenance, as well as paperwork. This severely limits the time budget that remains for assessing mental health issues such as depression and futher raises the barrier of accurate diagnosis and proper treatment \cite{telford2002}.

	Despite the severe consequences, depression is one of the most treatable mental illnesses but it is also one of the most under-diagnosed globally. 
	In general health-care, 48.4\% of patients suffering from depression go unrecognized \cite{jama2003}.

	\subsection{Motivation and Objective}
	The most recent study conducted by the Institute of Mental Health has shown that Major Depressive Disorder (MDD) is the most common mental disorder in Singapore. An estimated 1 in 17 people having suffered from MDD at some point in their lives which is almost twice that of the next most common disorder, alcohol abuse \cite{annacadmedsg}.

	In 2015, despite falling overall suicide rates, and a shrinking proportion of youths in the population of Singapore, the number of suicide deaths in youths between 10-19 of age was twice as many as the year before and it is also the highest in 15 years \cite{samaritansofsingapore2016}.
	
	In the past decade, there have been research successfully correlating emotion with voice production and speech acoustics \cite{uwa2001}. 
	Corollary to that, active research of late into the use of voice acoustics as predictors of clinical depression scores has seen success, proving that it is an effective indicator of depression severity \cite{jov2016}.\\
	
	\subsection{Important Requirements}
	...
	
	\section{Modelling and Approach}
	Leveraging on the success of the modelling of depression prediction of Personal Health Questionaire depression scale (PHQ-8) scores in project 1, we extend and apply the work done to solve the aforementioned problem of under-staffing.
	Our solution is to implement pre-screening and automate the process of the deciding if the person needs an appointment or otherwise.
	In this manner, we cut down on the number of appointments reducing workload on the staff. 
	Moreover, our process allows prioritizing of patient's appointments by their PHQ-8 score which represents the severity of depression.
	Hence, we would need to use two gaussian processes (GP): Gaussian Process Classifier (GPC) and the Gaussian Process Regressor (GPR). 
	In this section, we will firstly describe the process that we are proposing and we will go into detail on each component of the process. 
	We note that the process is a general framework and that different clinics have different operating processes, clinics should tweak the framework to better fit their needs. 
	Hence, we would omit some detail in our model and leave them to the implementer.

	\subsection{Automation Flow} \label{af}
	\begin{figure}[h]
 		\begin{center}
		\includegraphics[width=0.48\textwidth]{automation} 
  		\end{center}
  		\caption{Automation Flow}
  		\label{auto_flow} 
 	\end{figure}
 	\begin{comment}
	@startuml
		Patient -right-|>(Phone Interview) : seek treatment
		(Phone Interview) -right-|> [GPC] : voice recording
		(Set appointment) --> [GPC] : repeat process after a time t
		[GPC] --> (Schedule appointment) : < p
		[GPC] --> (Is patient depressed?) : >= p
		(Is patient depressed?) ..> (Schedule appointment ) : yes
		(Is patient depressed?) ..> (No appointment needed) : no
	@enduml
 	\end{comment}

	Figure \ref{auto_flow} represents the flow of a person who is seeking for medical attention from a psychiatric clinic.
	Prior to medical treatment, the patient would be calling up the clinic to arrange an appointment time and date. 
	Before the clinic fixes the appointment, a short phone interview consisting of several questions is conducted.
	Refer to the sample questions in the section titled `Proof of Concept'.
	The objective of the interview is to record the patient's audio signal as he/she answers the questions.
	The recordings will the serve as an input for GPC, which is a classifier that will output both a probability estimate and a label, determining if the patient is suffering from depression or not.
	The probability estimate represents the confidence on the predicted label. 
	We can then define a specific pre-defined probability estimate $\rho_c$, such that we have two two scenarios as listed below. 
	In the illustration below, we make the argument based on a probability estimate $p$ which is based on some audio recording collected.
	
	\begin{enumerate}
		\item {Confident of prediction ($p \geq \rho_c$)} \\
		GPC is confident of its prediction, allowing us to make decision with confidence on the predicted label.
		Then, we make the decision based on whether the patient is depressed or not depressed as predicted by GPC. 
		A depressed patient will be given an appointment(refer to Figure \ref{sch_app}) for treatment, whereas a non-depressed patient would be informed that he/she is not required to go for an appointment.
		\item {Not confident of prediction ($p < \rho_c$}) \\
		GPC is not confident of its prediction, we cannot rely on the predicted label. 
		In this case, patient can proceed with the appointment scheduling(Figure \ref{sch_app}). 
		We note that he/she would need to be diagosed by the clinician and treatment if appropriate. 
		We would also obtain the ground truth from the diagnosis and use that to train the GPC/GPR on this datapoint. 
	\end{enumerate}
	
	\begin{figure}[h]
 		\begin{center}
		\includegraphics[width=0.48\textwidth]{appointment} 
  		\end{center}
  		\caption{Flow for Scheduling an Appointment}
  		\label{sch_app} 
 	\end{figure}
 	\begin{comment}
	@startuml			
		[Voice recording] -right-|> [GPR] : input
		[GPR] -right-|> (PHQ-8 Score) : outputs
		(PHQ-8 Score) -right-|> [Scheduler] : feed to
		[Scheduler] --> (Appointment) : get time and date
		(Appointment) -left-|> (PHQ-8/Depression LBL)
		(PHQ-8/Depression LBL) -left-|> [Update GPC/GPR] : use to
	@enduml
 	\end{comment}

	Figure \ref{sch_app} represents the case that he/she needs an appointment. 
	Similarly, the patient's voice is use as an input for the GPR which predicts the PHQ-8 score. 
	PHQ-8 score determines the levels of depression (from $0$-$24$, $0$ means no depression and $24$ means very depressed) of the patient, allowing the staff to prioritize appointments based on the PHQ-8 score.
	We would prioritize patients who are more depressed (predicted by the GPR with a higher PHQ-8 score) will have higher priority and vice versa.
	Through the appointment(s) with the psychiatrist, the ground truth of the depression and PHQ-8 labels will be obtained and can be use to train both GPC and GPR if required (if this datapoint was predicted with no confidence i.e. $p < \rho_c$).
	As more data points are observed and subsequently use for training, both GPC's and GPR 's accuracy would improve incrementally.

	\subsection{Insights to Our Model}
	\subsubsection{Gaussian Processes :}
	In our modelling, we used two GPs for regression and classification tasks respectively. 
	The GP models that we used here are modelled similarly to the modelling done in project 1. 
	GPR predicts the PHQ-8 score, while GPC predicts whether the person is depressed or not. 
	Similar to project 1, we trained GPR and GPC with data obtained from Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \cite{avec2016}, with PHQ-8 and depression labels (provided as part of the dataset) respectively. 
	We applied the same audio signal processing techniques to the audio files as per project 1. 
	We used GP with a dot-product kernel, with the Mel Frequency Cepstral Coefficients (MFCC) feature subset as we have seen the good theorical and experimental results the feature subset had produced in project 1. 
	More importantly, we make the same two assumptions so that the GP model planning and decision making is suitable for depression prediction:
	\begin{enumerate}
		\item Depression prediction is an event-based which provides a single depression estimate over a time period \cite{Valstar2016}.
		\item Speech signals extracted from people suffering from depression should share some similarities and thus admissible for prediction with the Gaussian Process models \cite{Cummins2015}.
	\end{enumerate}
	In our formulation GPs must be able to train on new data. 
	Since the GP models recieve new data incrementally, we cannot use an offline GP as we have described in project 1. 
	Hence, we would need to tweak the GP to fit this problem, needing a GP that learn dynamically, adapting to new data as it becomes avaliable. 
	Online machine learning is a method that allows the data to be updated when it becomes available. 
	We have noted that there are online variants of GP which will be used in our project.
	
	\subsubsection{Phone Interview :}
	We noted that regardless of the questions asked, a depressed person will still exhibit signs of depression in his speech \cite{jad2008}. 
	Hence, the questions asked are irrelevant. 
	A sample of questions are presented in the section titled 'Proof of Concept'.

	\subsubsection{Decision making :}
	The greatest advantage of using GP over other machine learning algorithms is that it provides us with a probability estimate representing confidence of the prediction. 
	With that probability, we can decide if the predicted label can be relied on or that it cannot be trusted and needs to learn this data point. 
	In our model, we exploit this property, unique to GPs. 
	After the interview, GPC will predict the depression label with a confidence $p$ on the audio recording. 
	As mentioned in the modelling, we define a particular probability $\rho_c$. 
	If the GPC predicts with $p \geq \rho_c$, then we can trust the predicted label and continue to decide appropriately based on the label whereas if the GPC predicts with $p < \rho_c$, then we cannot trust the predicted label and refer the patient to an appointment with the psychiatrist to obtain more data.
	We determine the probability $\rho_c$ experimentally by observing the prediction quality of the labels in the training data, a summary is given in Table \ref{tab:rho}. We observe that with a $\rho_c = 70\%$, it would predict the depression label with $100\%$ accuraccy. 
	Henceforth, in this paper we will define $\rho_c = 70\%$.
	
 	\begin{table}[h]
 		\begin{center}
  			\begin{tabular}{ | r | c | c | }
    			\hline
			 	 \bfseries $p_c$($\%$)	& \bfseries Depression 	& \bfseries No Depression \\ \hline
				 $[0,50)$		& - 				& - 			 			\\ \hline
				 $[50,60)$		& $60\%(3/5)$ 	& $80\%(44/55)$ 	 	\\ \hline 
				 $[60,70)$		& $100\%(1/1)$ & $84.2\%(16/19)$ 	\\ \hline
				 $[70,80)$		& - 				& $100\%(4/4)$		 	\\ \hline
				 $[80,90)$		& - 				& $100\%(1/1)$ 	 	\\ \hline 
				 $[90,100]$		& - 				& - 			 			\\ \hline
				 -					& - 				& $90\%(9/10)$ 		\\ \hline
			 \end{tabular}
		\end{center}
 	\caption{Relationship between probability and accuraccy}
 	\label{tab:rho}
 	\end{table}

	\subsubsection{Scheduling :}
	For the cases when the person needs to be scheduled for an appointment with the psycharist, we can use the GPR to determine the predicted PHQ-8 scores. 
	With the predicted scores, we can prioritise certain higher risk individuals over the rest. 
	We were inspired by triage algorithms used in emergency services \cite{shah2015managing,oredsson2011systematic}, where priority of one's treatments are decided by the severity of their ailments \cite{wiki:Triage}.
	We have seen that this method dispenses limited resources with efficiency \cite{rosedale2011effectiveness} throughout many emergency services world-wide. 
	Similarly, we want to apply that idea into psychiatric clinic, which are also facing a shortage of resources. 
	Since the depression clinic is understaffed and struggling to keep up with number of patients, it is wise to piroritize the individuals who are predicted with higher PHQ-8 scores, which indicates that they are likely to be more depressed. 
	Hence, we optimize the scheduling of appointments, piroritizing people with higher predicted PHQ-8 scores.

	\subsection{Qualitative Advantages}
	The greatest advantage of using a GP is that it provides us with a probability estimate along with the predicted label. 
	We rely on the probability to determine the reliability of the predicted label. 
	As we are dealing with human beings, we would want to rely on the predicted label only if it is reliable. 
	Additionally, we have read in medical literature regarding the dire consequences of misdiagnosis and/or inappropriate treatment \cite{nasrallah2015consequences,bowden2001strategies,dunner2003clinical} in the area of depression. 
	With the probability, we can be confident of the decisions that we make, that can potentially affect a person. 
	Assume that we have a predicted label that can be trusted, then with the label, i.e. true for depressed and false for not depressed, we can decide if the person needs to come to the clinic for an appointment. 
	Hence, reducing the number of appointments that are made, easing the workload of the staff in the clinic. \\
	
	In addition to reducing the workload of the staff, the GP models can potentially improve as more patients go through the pre-screening process.
	It can be the case that the GP is not confident of its prediction, then we should not trust the label that the GP had predicted. 
	Then, we would need to determine by the means of a physical examiniation if the person in question is depressed or not and administer the appropriate treatment. 
	From the appointment with the psychiatrist, he/she can determine if the patient is depressed or not. 
	With the ground truth, we can now train the GPs with the new data point. 
	We are also careful to only update the GP if the data point is predicted with low confidence. \\

	In our model, we do not require a trained staff to administer the phone interview. 
	GP will determine the depression label of the person objectively, not considering the content of the interview but relying on certain depression indicators in speech \cite{nimh2015}. 
	This allows the removal of any bias (eg. gender, racial) in pre-screening, which can potentially cause misdiagnosis. 
	Additionally, the clinic can save skilled-manpower as they can hire anyone or can use an automated system to perform the pre-screening task.
	We also prioritize the clinic's resources based on a person's depression severity, allowing better allocation of resources. 
	This ensures that the clinic's resources are directed to people who need them most. \\

	Hence, our model introduces an unbiased pre-screening process reaping the following benefits: leading to a reduction of the number of appointments, reduction of manpower required, a pre-screening process whose accuracy improves incrementally over time, and a better allocation of resources.

	\section{Evaluation}
	In order to test our proposed GP models, we conducted tests on data obtained from Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \cite{avec2016}. 
	The goal of AVEC is to weigh-in on the various approaches used to recognize emotions under unambiguous conditions. 
	AVEC 2016 provided 2 pieces of data as input: visual and auditory data from each of the participants. 
	However, we would be reducing the scope of the experiment, limiting the experiment to only the auditory data. 
	Two Sub-Challenges were lised in AVEC 2016. 
	We are only interested in the Depression Classification Sub-Challenge, which requires participants to classify inputs by the PHQ-8 score. 
	In this experiment, we would be using the audio data along with their corresponding PHQ-8 scores to test our assumptions and confirm our hypothesis.

	\subsection{Data}
	\begin{figure}[h]
	\center
 	\includegraphics[width=0.5\textwidth]{histogram_phq8}
	\caption{Histogram of the PHQ-8 scores}
	\label{histogram_phq8}
	\end{figure}
	
	The depression data used in AVEC 2016 was obtained from the benchmarking database, Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ). 
	Data collected from DAIC-WOZ include raw audio and video recordings and the corresponsing PHQ-8 score (from 0 to 24) \cite{jad2008}. Hence, we would 
	need to pre-process the auditory data before we use it in our experiment. The pre-processing is briefly discussed in the section below.
	The distribution of the depression severity scores in the dataset is given in Figure \ref{histogram_phq8}. 
	The data provided are split into 2 sets: training and development.
	An overview of the data is given in Table \ref{summary_table}.

 	\begin{table}[h]
 		\begin{center}
  			\begin{tabular}{ | r | c | c | c | }
    			\hline
			 		& \bfseries Training	& \bfseries Development 	& \bfseries All \\ \hline
			 $n$		& 95 			& 31 				& 126 \\ \hline
			 $\mu$	& 6.326 		& 7.548			& 6.626 \\ \hline
			 $\sigma$	& 5.597 		& 6.690 			& 5.909 \\ \hline
			 \end{tabular}
		\end{center}
 	\caption{Summary of Datasets provided}
 	\label{summary_table}
 	\end{table}

	\subsection{Pre-processed data}
	Since the focus of this paper is the prediction of the PHQ-8 score, we will not describe the pre-processing step in detail.
	We used standard signal processing techniques to extract the 4 audio feature sets (Energy, MFCC, Magnitude Spectrum, Zero-crossing) as presented in the Modelling 
	and Approach section.
	Each audio feature set comprises of several individual features and the breakdown of the actual number of feature columns is summarized in Table \ref{no_features}.

 	\begin{table}[h]
 		\begin{center}
  			\begin{tabular}{ | r | c | }
    			\hline
			 \bfseries Audio Feature Sets 	& \bfseries Number of features \\ \hline
			 Magnitude Spectrum		& 512 \\ \hline
			 MFCC 				& 12 \\ \hline
			 Energy 				& 1 \\ \hline
			 Zero-Crossing Rate 		& 1 \\ \hline
			 \bfseries Total			& \bfseries 526 \\ \hline
			 \end{tabular}
		\end{center}
 	\caption{Number of features extracted}
 	\label{no_features}
 	\end{table}

 	\subsection{Measure of Accuracy}
	AVEC 2016 provided a baseline classifier that consistently predicts the PHQ-8 score with $\text{RMSE}=6.7418$ \cite{avec2016}. 
	In order to provide a meaningful and consistent comparison to the baseline provided, we used the same Root Mean Square Deviation Error (RMSE) to measure the 
	error rate on both Training and Development datasets. 
	RMSE (Equation \ref{eq:rmse}) is a commonly used in the machine learning community to measure the differences between the values predicted by a model 
	and the ground truth \cite{Dhanani:EECS-2014-131}. 
	\begin{equation}\label{eq:rmse}
  	\text{RMSE} = \sqrt{\frac{\sum_{t=1}^n (\hat y_t - y_t)^2}{n}}
 	\end{equation} 

	\subsection{Feature Selection}
	Feature selection is the process of selecting a subset of relevant features including variables or predictors to be used in a model for machine learning. 
	The purpose of feature selection is to reduce the complexity of a model to more easily be interpreted. 
	The benefit is three-fold: improving the prediction performance of the predictors, providing faster and most cost-effective predictors, 
	and providing a better understanding of the underlying process that generated the data \cite{Guyon2003}.\\
	
	Since we have more features than data points, it tends to lead to overfitting \cite{Smith2011}. 
	Therefore feature selection is first performed on the data before applying machine learning. 
	The feature selection algorithms used are popular and are taken from scikit-feature, a feature selection library \cite{li2016feature}: CIFE \cite{Lin2006}, Relief 	
	\cite{Rob2003}, CFS \cite{HALLHALL}. 
	We will not go into detail as feature selection is not the main focus of the report.
	
	\subsection{Experimental Setup}
	We compared the proposed GP models against state-of-the-art machine learning models as mentioned in the previous section. 
	For the ease of testing, all implementations of the algorithms except for GP ARD come from the popular machine learning library, Scikit Learn \cite{scikit-learn}. 
	We used the implementation of GP ARD from GPy, a Gaussian Processes framework in Python \cite{gpy2014}.
	The hyper-parameters are either determined by the defaults used in either libraries or some reasonable defaults were used.
	Each machine learning model is trained against the training set and thereafter tested against the development set using RMSE as the error metric. 
	The entire experimental process is shown in Figure \ref{process}. \\
 
 	\begin{comment}
	\begin{table}[h]
  		\begin{center}
   			\begin{tabular}{ | r | c |}
	    		\hline
			Algorithm & Hyper-parameters \\ \hline\hline
			K-Nearest Neighbors        & x \\ \hline
			SVM - Linear               & x \\ \hline
			SVM - RBF                  & x \\ \hline
			Decision Tree              & x \\ \hline
			Random Forest              & x \\ \hline
			AdaBoost                   & x \\ \hline
			Naive Bayes                & x \\ \hline
			GP ARD  	           & x \\ \hline
			GP Isotropic Matern        & x \\ \hline
			GP Dot Product 	           & x \\ \hline
			\end{tabular}
		\end{center}
		\caption{List of Machine Learning Algorithms with their corresponding hyper-parameters}
		\label{list_mls}
	\end{table}
 	\end{comment}
 	
	\begin{figure}[h]
 		\begin{center}
		\includegraphics[width=0.48\textwidth]{process} 
  		\end{center}
  		\caption{Experimental process}
  		\label{process} 
 	\end{figure}
 	\begin{comment}
	@startuml
	digraph g {
		graph [rankdir=LR];
	 	ML,RMSE[shape=square];
		Model[shape=circle];
		ap[shape=rect label="Audio Processing"];
		Train_AudioData,Dev_AudioData[shape=egg];
		Train_AudioData -> ap;
		ap -> Train_X;
		ap -> Train_y;
		ap -> Dev_X;
		Dev_AudioData -> ap;
		ap -> Dev_y;
		Train_X -> ML;
		Train_y -> ML;
		ML -> Model
		Dev_X -> Model
		Model -> RMSE[label=Prediction]
		Dev_y -> RMSE[label=Actual]
	}
	@enduml
 	\end{comment}

	\subsection{Results}
	We first ran the experiment across the dataset using all 526 features, without feature selection. 
	As we would expect \cite{Cawley2014}, the results are unacceptable as the ratio of the number of features to the number of data points is too high,
	resulting in possible overfitting. 
	The results of the initial experiment is illustrated in Figure \ref{results_all}. 
	We would expect the GP ARD would be able to theoritcally extract relevant features and improve prediction.
	However, we have observed experimentally that GP ARD performs poorly, along with other GPs. \\

	\begin{figure}[h]
 		\begin{center}
		\includegraphics[width=0.5\textwidth]{results_all} 
  		\end{center}
  		\caption{Results across all features}
  		\label{results_all}
 	\end{figure}
	
	We repeated the experiment with feature selection and ran each of the feature subset gathered from the feature selection algorithms against each of the machine 
	learning algorithms. 
	We observed that Relief, CIFE and CFS selected a large number of MFCC features. 
	The number of features in each feature subset is shown in Table \ref{feat_subset}. 
	This confirms our assumption that MFCC gives the best predictive power in PHQ-8 depression severity prediction. 
	Hence, we also ran the experiment using only MFCC features. 
	The best results across all feature subsets are shown in Figure \ref{results_best} and in Table \ref{rmse_results}. The line shown across the 
	bar chart represents the baseline RMSE provided. \\

 	\begin{table}[h]
 		\begin{center}
  			\begin{tabular}{ | r | c | }
    			\hline
			 \bfseries Feature Selection 	&\bfseries Number of features \\ \hline
			 MFCC           			& 12 \\ \hline
			 CIFE        				& 3 \\ \hline
			 Relief 				& 23 \\ \hline
			 CFS 					& 6 \\ \hline
			 All 					& 526 \\ \hline
			 \end{tabular}
		\end{center}
 	\caption{Feature subsets}
 	\label{feat_subset}
 	\end{table}

	\begin{figure}[h]
 		\begin{center}
		\includegraphics[width=0.5\textwidth]{results_best} 
  		\end{center}
  		\caption{Best Results across all feature subsets}
  		\label{results_best}
 	\end{figure}

	\begin{table}[h!]
		\begin{center}
			\begin{tabular}{ | r | c | c | c | }
			\hline
			\multirow{2}{*}{\bfseries Algorithm}	& \multirow{2}{*}{\bfseries Subset}	& \multicolumn{2}{c|}{\bfseries RMSE} \\\cline{3-4}
									&						& \bfseries Train 	& \bfseries Dev \\ \hline
			\bfseries GP Dot Product  	& \bfseries MFCC  & \bfseries 5.03 & \bfseries 6.34 \\ \hline
			AdaBoost              				& Relief					& 3.55 		& 6.52 \\ \hline
			K-Nearest Neighbors   			& CFS 					& 3.75 		& 6.53 \\ \hline
			SVM - Linear          				& MFCC  					& 5.29 		& 6.63 \\ \hline
			Random Forest         			& CFS   					& 5.61 		& 6.75 \\ \hline
			Decision Tree         				& MFCC  					& 5.60 		& 6.80 \\ \hline
			SVM - RBF             				& CIFE  					& 4.91 		& 6.91 \\ \hline
			GP Isotropic Matern   			& CFS   					& 0.00 		& 7.18 \\ \hline 
			Naive Bayes           				& Relief					& 6.87 		& 7.59 \\ \hline
			GP ARD                				& All   						& 3.72 		& 10.09 \\ \hline

			\end{tabular}
		\end{center}
		\caption{RMSE Results}
		\label{rmse_results}
	\end{table}
	
	As expected, the models perform better with the MFCC feature set. 
	Unexpectedly, the simple GP dot product model, trained with 12 features and 95 data points, outperforms all other machine learning models in our tests. 
	Our results also confirms the initial assumption that MFCC is an appropriate feature set to be used in emotion and therefore depression prediction and that GP is 
	applicable and feasible in predicting PHQ-8 scores.

	\section{Conclusion}	
	Our work has succesfully shown that GP is a good model for this problem and can predict PHQ-8 better than state-of-the-art machine learning models. 
	In addition to being on par or better at prediction, GP can inherently provide an estimate of prediction uncertainty. 
	This allows the user to gauge the model's confidence of the prediction, and to make more informed decisions based on both the prediction and its uncertainty. 
	We can also intelligently supplement more data to our training set based on the prediction uncertaintly. 
	Therefore, after considering both results and GP's advantages, we conclude the GP Dot Product trained using MFCC feature set is a good model for 
	depression prediction.
	
	\section{Further Work}
	For this experiment, we only used machine learning algorithms with their default parameters. 
	An aspect that deserves further exploration is to perform automatic hyper-parameter optimization
	across all the machine learning algorithms to fine-tune each model's performance. 
	In particular, we can try Hyperopt-sklearn \cite{Komer2014HyperoptsklearnAH} or GP based hyper-parameter tuner. 
	We opine that with hyper-parameter tuning, we can predict PHQ-8 scores better and can have a more objective comparison of the different learning algorithms.
	
	\section{Contributions}
	\begin{itemize}
		\item \textbf{Antoine Charles Vincent Garcia}: 
		Scripting the program, setting up machine learning libraries, running tests and generation of the utility function.
		\item \textbf{Chan Jun Wei}: 
		Scripting the program, setting up machine learning libraries, running tests and generation of the utility function.
		\item \textbf{Chen Tze Cheng}: 
		Scripting the program, setting up machine learning libraries, running tests and generation of the utility function.
		\item \textbf{Eric Ewe Yow Choong}: 
		Formatting the report as well as research and writing up of the technical approach section.
		\item \textbf{Han Liang Wee, Eric}: 
		Retrieving data, testing as well as research and writing up of the technical approach section.
		\item \textbf{Ho Wei Li}: 
		Research, vetting of the report and writing up of the motivation and introduction of the experiment. \\
	\end{itemize}
	
	\bibliographystyle{aaai}
	{\scriptsize \bibliography{references}}

\end{document}
